{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2. Dealing with Missing Values.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "17K8g2NDKWdOIXEFjTjcvCNnzhqvDT6C6",
      "authorship_tag": "ABX9TyMe1G2BK62uWa7AQ9nWt/ZP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cirilwakounig/MachineLearning/blob/main/2_Dealing_with_Missing_Values.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOrnVotduYiq"
      },
      "source": [
        "# Dealing with Missing Values\r\n",
        "\r\n",
        "This script is showing how to effectively deal with missing values. More information about methods and strategies can be found here: https://www.kaggle.com/alexisbcook/missing-values. This script is based on the course 'Intermediate Machine Learning' provided by Kaggle. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xvAALunuTkq"
      },
      "source": [
        "# Import the required Libraries\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.ensemble import RandomForestRegressor\r\n",
        "from sklearn.metrics import mean_absolute_error\r\n",
        "from sklearn.impute import SimpleImputer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vSa6KAMvtDf"
      },
      "source": [
        "##### 1. Import the required Data\r\n",
        "\r\n",
        "---\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1yKn1sdvxcL"
      },
      "source": [
        "# Import the Data Set\r\n",
        "file_path_train = '/content/drive/MyDrive/Colab Notebooks/Kaggle Course/Intermediate Machine Learning/train.csv'\r\n",
        "file_path_test = '/content/drive/MyDrive/Colab Notebooks/Kaggle Course/Intermediate Machine Learning/test.csv'\r\n",
        "\r\n",
        "# Read the data\r\n",
        "X_full = pd.read_csv(file_path_train, index_col = 'Id')\r\n",
        "X_test_full = pd.read_csv(file_path_test, index_col = 'Id')\r\n",
        "\r\n",
        "# Assign the dependent variable - Remove missing target values\r\n",
        "X_full.dropna(axis = 0, subset = ['SalePrice'], inplace = True)   # Inplace = True overrides existing data frame\r\n",
        "y = X_full.SalePrice\r\n",
        "\r\n",
        "# Separate features from predictors\r\n",
        "X_full.drop(['SalePrice'], axis = 1, inplace = True)\r\n",
        "\r\n",
        "# Assign features - To keep things simple, we'll use only numerical predictors\r\n",
        "X = X_full.select_dtypes(exclude=['object'])\r\n",
        "X_test = X_test_full.select_dtypes(exclude=['object'])\r\n",
        "\r\n",
        "# Split the data in train and validation sets\r\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, train_size = 0.8, test_size = 0.2, random_state = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxVQHKCkxcMW"
      },
      "source": [
        "#### 2. Set up Regressor used to score Data processing Approaches\r\n",
        "\r\n",
        "---\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKH473lUxbHt"
      },
      "source": [
        "# We are using a Random Forest Regressor to score the performance\r\n",
        "\r\n",
        "def score_mae(X_train, X_val, y_train, y_val):\r\n",
        "  # Develop Model and make Predictions\r\n",
        "  model = RandomForestRegressor(n_estimators = 100, random_state = 0)\r\n",
        "  model.fit(X_train,y_train)\r\n",
        "  preds = model.predict(X_val)\r\n",
        "\r\n",
        "  error = mean_absolute_error(y_val, preds)\r\n",
        "  return (error)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUF3sNXhykIk"
      },
      "source": [
        "#### 3. Test Data Processing Approaches\r\n",
        "\r\n",
        "---\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTdO7pWH5J0o"
      },
      "source": [
        "# Variable Containing the Score of each Approach\r\n",
        "method_score = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nw32P6L_yqni"
      },
      "source": [
        "##### 3.1 Drop Columns with Missing Values\r\n",
        "\r\n",
        "This approach drops any column, that contains a missing value.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nhwe7dYIyvew",
        "outputId": "41c0eb50-555e-4b82-ae59-e1b9d3c55fb6"
      },
      "source": [
        "# Detect columns with missing values\r\n",
        "drop_cols = [col for col in X_train.columns if X_train[col].isnull().any()]   # any() is the keyword to drop any column containing a missing value. \r\n",
        "\r\n",
        "# Remove columns with missing values\r\n",
        "reduced_X_train = X_train.drop(drop_cols, axis = 1)\r\n",
        "reduced_X_val = X_val.drop(drop_cols, axis = 1)\r\n",
        "\r\n",
        "# Count the number of missing rows in each column\r\n",
        "print(X_train.shape)\r\n",
        "numMissingValues = X_train.isnull().sum()\r\n",
        "print(numMissingValues[numMissingValues>0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1168, 36)\n",
            "LotFrontage    212\n",
            "MasVnrArea       6\n",
            "GarageYrBlt     58\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U26_8YrG4kBt",
        "outputId": "b1794e92-c199-4d93-c274-dd881689682b"
      },
      "source": [
        "# Predict using the reduced feature set\r\n",
        "method_score.append(score_mae(reduced_X_train, reduced_X_val, y_train, y_val))\r\n",
        "print(method_score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[17837.82570776256]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hd8vYJa5ZSF"
      },
      "source": [
        "##### 3.2 Impute Missing Values based on Similarities\r\n",
        "\r\n",
        "This approach uses sklearn's impute function to approximate missing values. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTRTmQp56PKf"
      },
      "source": [
        "# Define the imputer\r\n",
        "my_imputer = SimpleImputer()\r\n",
        "\r\n",
        "# Impute using the train features as a fitting set for the imputer\r\n",
        "imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))   # Transform np array to data frame\r\n",
        "imputed_X_val = pd.DataFrame(my_imputer.transform(X_val))\r\n",
        "\r\n",
        "# Imputation returns np array and thus removes columns. Reassign column names here\r\n",
        "imputed_X_train.columns = X_train.columns\r\n",
        "imputed_X_val.columns = X_val.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rk6_z50s7tNO",
        "outputId": "685fb887-0588-45b5-c89d-aa1ce1ff26b6"
      },
      "source": [
        "# Predict using imputed dataset\r\n",
        "method_score.append(score_mae(imputed_X_train, imputed_X_val, y_train, y_val))\r\n",
        "print(method_score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[17837.82570776256, 18062.894611872147]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qthj5NAP8DCV"
      },
      "source": [
        "##### 3.3 An Extension to Imputation\r\n",
        "\r\n",
        "Here, the imputation approach from 3.2 is extended by keeping track of which values have been imputed. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Q3_CQoU8MVT"
      },
      "source": [
        "# Make a copy to avoid changing data when imputing \r\n",
        "X_train_plus = X_train.copy()\r\n",
        "X_val_plus = X_val.copy()\r\n",
        "\r\n",
        "# Make new columns indicating what will be imputed. df[col].isnull() returns true/false in each row of column col in dataframe df\r\n",
        "for col in drop_cols:\r\n",
        "  X_train_plus[col + '_was_missing'] = X_train_plus[col].isnull()\r\n",
        "  X_val_plus[col + '_was_missing'] = X_val_plus[col].isnull()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUjdh-mD9XYP"
      },
      "source": [
        "# Define the imputer\r\n",
        "my_imputer = SimpleImputer()\r\n",
        "\r\n",
        "# Impute using the train features as a fitting set for the imputer\r\n",
        "imputed_X_train_plus = pd.DataFrame(my_imputer.fit_transform(X_train_plus))   # Transform np array to data frame\r\n",
        "imputed_X_val_plus = pd.DataFrame(my_imputer.transform(X_val_plus))\r\n",
        "\r\n",
        "# Imputation returns np array and thus removes columns. Reassign column names here\r\n",
        "imputed_X_train_plus.columns = X_train_plus.columns\r\n",
        "imputed_X_val_plus.columns = X_val_plus.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxMiP27N9rii",
        "outputId": "e0d7d1a6-41aa-415f-ba77-cd77497734ef"
      },
      "source": [
        "method_score.append(score_mae(imputed_X_train_plus, imputed_X_val_plus, y_train, y_val))\r\n",
        "print(method_score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[17837.82570776256, 18062.894611872147, 18148.417180365297]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKbinFX3_R2D"
      },
      "source": [
        "##### 3.4 Conclusion\r\n",
        "\r\n",
        "As the share of missing values is very low, dropping whole columns is unwise, as it would remove valuable information that could be used to better fit the model. Dropping whole columns only makes sense, if a significant amount of entries are missing within a column. In the case here, dropping columns proved to be the best approach, as it always must be considered, if imputing values would make sense. "
      ]
    }
  ]
}